{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore')\n",
    "import retro\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten, Permute\n",
    "import tensorflow.keras.backend as K\n",
    "from collections import deque\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from skimage import transform # Help us to preprocess the frames\n",
    "from skimage.color import rgb2gray # Help us to gray our frames\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = retro.make(game='Airstriker-Genesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of our frame is:  Box(224, 320, 3)\n",
      "The action size is :  12\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of our frame is: \", env.observation_space)\n",
    "print(\"The action size is : \", env.action_space.n)\n",
    "\n",
    "# Here we create an hot encoded version of our actions\n",
    "# possible_actions = [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0]...]\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Greyscale frame \n",
    "    gray = rgb2gray(frame)\n",
    "    \n",
    "    # Crop the screen (remove the part below the player)\n",
    "    # [Up: Down, Left: right]\n",
    "    cropped_frame = gray[8:-12,4:-12]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    # Thanks to Miko≈Çaj Walkowiak\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [102,152])\n",
    "    \n",
    "    return preprocessed_frame # 110x84x1 frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((102,152), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((102,152), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = [102, 152, 4]      # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels) \n",
    "action_size = env.action_space.n # 8 possible actions\n",
    "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
    "\n",
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 50000  # How many last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 1000  # Minimum number of steps in a memory to start training\n",
    "MINIBATCH_SIZE = 32  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 1  # Terminal states (end of episodes)\n",
    "SAVE_MODEL_EVERY = 2000\n",
    "MODEL_NAME = 'acrobot'\n",
    "MIN_REWARD = -200  # For model save\n",
    "MEMORY_FRACTION = 0.20\n",
    "\n",
    "# Environment settings\n",
    "EPISODES = 10\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # not a constant, going to be decayed\n",
    "EPSILON_DECAY = 0.999\n",
    "MIN_EPSILON = 0.1\n",
    "\n",
    "#  Stats settings\n",
    "SHOW_PREVIEW=False\n",
    "LOAD_MODEL=None\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    AGGREGATE_STATS_EVERY = 500  # episodes\n",
    "else:\n",
    "    AGGREGATE_STATS_EVERY = 500  # episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, model=None):\n",
    "        \n",
    "        self.model= self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "\n",
    "    def create_model(self):\n",
    "        if LOAD_MODEL is not None:\n",
    "            print(\"Loading model\")\n",
    "            model = load_model(LOAD_MODEL)\n",
    "            print(\"Model loaded\")\n",
    "        else:\n",
    "            model = Sequential()\n",
    "            model.add(Conv2D(16, (8, 8), strides=(4, 4),input_shape=state_size))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Conv2D(32, (4, 4), strides=(2, 2)))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(256))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(Dense(env.action_space.n, activation=\"linear\"))\n",
    "            model.compile(loss=\"mse\", optimizer='rmsprop', metrics=[\"accuracy\"])\n",
    "        return model\n",
    "    \n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "        \n",
    "    def train(self, terminal_state, step):\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "        \n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        \n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "        \n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
    "        \n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for index, (current_state, action ,reward, new_current_state, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT*max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "                \n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "            \n",
    "        self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE,\n",
    "                       verbose=0, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n",
    "        \n",
    "        # Determine if we want to update target_model\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "            \n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "            \n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?episodes/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DQNAgent' object has no attribute 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-51518e14d429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Every step we update replay memory and train main network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_replay_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-f659b2dc7bc0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, terminal_state, step)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE,\n\u001b[0;32m---> 64\u001b[0;31m                        verbose=0, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Determine if we want to update target_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DQNAgent' object has no attribute 'tensorboard'"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent()\n",
    "\n",
    "# Iterate over episodes\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "\n",
    "    # Reset environment and get initial state\n",
    "    current_state = env.reset()\n",
    "    current_state, stacked_frames = stack_frames(stacked_frames, current_state, True)\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # Epsilon greedy exploration policy\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, lives = env.step(action)\n",
    "        new_state, stacked_frames = stack_frames(stacked_frames, new_state, False)\n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
    "            env.render()\n",
    "\n",
    "        # Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "\n",
    "        # Save model, but only when min reward is greater or equal a set value\n",
    "        if not episode % SAVE_MODEL_EVERY:\n",
    "            agent.model.save('models/{MODEL_NAME}__{mr:_>7.2f}max_{avr:_>7.2f}avg_{minr:_>7.2f}min__{ts}.model'.format(MODEL_NAME=MODEL_NAME, mr=max_reward, avr=average_reward, minr=min_reward, ts=int(time.time())))\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
